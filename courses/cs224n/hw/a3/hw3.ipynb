{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import  torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# CS 224n Assignment #3: Dependency Parsing\n",
    "\n",
    "In this assignment, you will build a neural dependency parser using PyTorch. In Part 1, you will learn\n",
    "about two general neural network techniques (Adam Optimization and Dropout) that you will use to build\n",
    "the dependency parser in Part 2. In Part 2, you will implement and train the dependency parser, before\n",
    "analyzing a few erroneous dependency parses.\n",
    "\n",
    "## 1. Machine Learning & Neural Networks (8 points)\n",
    "\n",
    "\n",
    "(^1) Kingma and Ba, 2015,https://arxiv.org/pdf/1412.6980.pdf\n",
    "(^2) The actual Adam update uses a few additional tricks that are less important, but we won’t worry about them here.\n",
    "(^3) Srivastava et al., 2014,https://www.cs.toronto.edu/ ̃hinton/absps/JMLRdropout.pdf\n",
    "\n",
    "\n",
    "i. (2 points) What mustγequal in terms of pdrop? Briefly justify your answer.\n",
    "\n",
    "ii. (2 points) Why should we apply dropout during training but not during evaluation?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Neural Transition-Based Dependency Parsing (42 points)\n",
    "\n",
    "\n",
    "In this section, you’ll be implementing a neural-network based dependency parser, with the goal of maximizing performance on the UAS (Unlabeled Attachemnt Score) metric.\n",
    "\n",
    "\n",
    "A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between\n",
    "headwords, and words which modify those heads. Your implementation will be atransition-basedparser,\n",
    "which incrementally builds up a parse one step at a time. At every step it maintains a partial parse,\n",
    "which is represented as follows:\n",
    "\n",
    "- A stack of words that are currently being processed.\n",
    "- A buffer of words yet to be processed.\n",
    "- A list of dependencies predicted by the parser.\n",
    "\n",
    "\n",
    "Initially, the stack only contains ROOT, the dependencies list is empty, and the buffer contains all words\n",
    "of the sentence in order. At each step, the parser applies a transition to the partial parse until its buffer\n",
    "is empty and the stack size is 1. The following transitions can be applied:\n",
    "\n",
    "```\n",
    "- SHIFT: removes the first word from the buffer and pushes it onto the stack.\n",
    "- LEFT-ARC: marks the second (second most recently added) item on the stack as a dependent of\n",
    "    the first item and removes the second item from the stack.\n",
    "- RIGHT-ARC: marks the first (most recently added) item on the stack as a dependent of the second\n",
    "    item and removes the first item from the stack.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On each step, your parser will decide among the three transitions using a neural network classifier.\n",
    "\n",
    "(a) (6 points) Go through the sequence of transitions needed for parsing the sentence“I parsed this\n",
    "sentence correctly”. The dependency tree for the sentence is shown below. At each step, give the\n",
    "configuration of the stack and buffer, as well as what transition was applied this step and what new\n",
    "dependency was added (if any). The first three steps are provided below as an example.\n",
    "\n",
    "\n",
    "![dependency diagram](./img/a3-2-a-1.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![dependency diagram](./img/a3-2-a-1.png)\n",
    "\n",
    "\n",
    "Stack | Buffer | New  dependency | Transition\n",
    "----  |  ----   | ----| ----\n",
    "[ROOT] | [I, parsed, this, sentence, correctly] | | Initial Configuration\n",
    "[ROOT, I]| [parsed, this, sentence, correctly]| | SHIFT\n",
    "[ROOT, I, parsed] | [this, sentence, correctly] | | SHIFT\n",
    "[ROOT, parsed] | [this, sentence, correctly] | parsed→I |LEFT-ARC\n",
    "[ROOT, parsed, this] | [ sentence, correctly] |  | SHIFT\n",
    "[ROOT, parsed, this, sentence] | [ correctly] |  | SHIFT\n",
    "[ROOT, parsed, sentence] | [ correctly] | sentence→this | LEFT-ARC\n",
    "[ROOT, parsed] | [ correctly] | parsed→sentence | RIGHT-ARC\n",
    "[ROOT, parsed, correctly] | [] |  | SHIFT\n",
    "[ROOT, parsed] | [] | parsed→correctly | RIGHT-ARC\n",
    "[ROOT] | [] | ROOT→parsed | RIGHT-ARC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(b) (2 points) A sentence containing n words will be parsed in how many steps (in terms of n )? Briefly\n",
    "explain why.  \n",
    "\n",
    ">2N + 1\n",
    "\n",
    "\n",
    "    1 initial configuration\n",
    "    Then, For each word:\n",
    "        1 Step for shifting from Buffer to Stack\n",
    "        1 Steps for applying transition / dependency\n",
    "    Total = 2N + 1\n",
    "\n",
    "> Complexity = O(N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(c) (6 points) Implement the init and parse step functions in the PartialParse class in\n",
    "`parser_transitions.py`. This implements the transition mechanics your parser will use. You\n",
    "can run basic (non-exhaustive) tests by running python `parser_transitions.py partc`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     1,
     4
    ],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class PartialParse(object):\n",
    "    def __init__(self, sentence):\n",
    "        \"\"\"Initializes this partial parse.\n",
    "\n",
    "        @param sentence (list of str): The sentence to be parsed as a list of words.\n",
    "                                        Your code should not modify the sentence.\n",
    "        \"\"\"\n",
    "        # The sentence being parsed is kept for bookkeeping purposes. Do not alter it in your code.\n",
    "        self.sentence = sentence\n",
    "\n",
    "        ### YOUR CODE HERE (3 Lines)\n",
    "        ### Your code should initialize the following fields:\n",
    "        ###     self.stack: The current stack represented as a list with the top of the stack as the\n",
    "        ###                 last element of the list.\n",
    "        ###     self.buffer: The current buffer represented as a list with the first item on the\n",
    "        ###                  buffer as the first item of the list\n",
    "        ###     self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
    "        ###             tuples where each tuple is of the form (head, dependent).\n",
    "        ###             Order for this list doesn't matter.\n",
    "        ###\n",
    "        ### Note: The root token should be represented with the string \"ROOT\"\n",
    "        ###\n",
    "        self.stack = [\"ROOT\"]\n",
    "        self.buffer = self.sentence.copy()\n",
    "        self.dependencies = []\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "\n",
    "    def parse_step(self, transition):\n",
    "        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
    "\n",
    "        @param transition (str): A string that equals \"S\", \"LA\", or \"RA\" representing the shift,\n",
    "                                left-arc, and right-arc transitions. You can assume the provided\n",
    "                                transition is a legal transition.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~7-10 Lines)\n",
    "        \n",
    "        ### TODO:\n",
    "        ###     Implement a single parsing step, i.e. the logic for the following as\n",
    "        ###     described in the pdf handout:\n",
    "        ###         1. Shift\n",
    "        ###         2. Left Arc\n",
    "        ###         3. Right Arc\n",
    "        if transition == 'S':\n",
    "            # remove last item in the stack\n",
    "            self.stack.append(self.buffer.pop(0))\n",
    "        elif transition == 'LA':\n",
    "            #  [..., n-1, n] :  n -> n-1\n",
    "            popped = self.stack.pop(-2)\n",
    "            self.dependencies.append((self.stack[-1], popped))\n",
    "        elif transition == 'RA':\n",
    "            #  [..., n-1, n] :  n-1 -> n\n",
    "            popped = self.stack.pop(-1)\n",
    "            self.dependencies.append(( self.stack[-1], popped))\n",
    "            \n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def parse(self, transitions):\n",
    "        \"\"\"Applies the provided transitions to this PartialParse\n",
    "\n",
    "        @param transitions (list of str): The list of transitions in the order they should be applied\n",
    "\n",
    "        @return dsependencies (list of string tuples): The list of dependencies produced when\n",
    "                                                        parsing the sentence. Represented as a list of\n",
    "                                                        tuples where each tuple is of the form (head, dependent).\n",
    "        \"\"\"\n",
    "        for transition in transitions:\n",
    "            self.parse_step(transition)\n",
    "            logger.debug(f'performing transition {transition} for sentence {self.sentence}')\n",
    "        return self.dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(d) (6 points) Our network will predict which transition should be applied next to a partial parse. We\n",
    "could use it to parse a single sentence by applying predicted transitions until the parse is complete.\n",
    "However, neural networks run much more efficiently when making predictions aboutbatchesof data\n",
    "at a time (i.e., predicting the next transition for any different partial parses simultaneously). We\n",
    "can parse sentences in minibatches with the following algorithm.\n",
    "\n",
    "---\n",
    "```\n",
    "Algorithm 1 Minibatch Dependency Parsing\n",
    "Input: sentences, a list of sentences to be parsed andmodel, our model that makes parse decisions\n",
    "\n",
    "Initializepartialparsesas a list of PartialParses, one for each sentence insentences\n",
    "Initializeunfinishedparsesas a shallow copy ofpartialparses\n",
    "while unfinished_parses is not empty do\n",
    "    Take the first batch size parses in unfinished_parsesas a minibatch\n",
    "    Use themodelto predict the next transition for each partial parse in the minibatch\n",
    "    Perform a parse step on each partial parse in the minibatch with its predicted transition\n",
    "    Remove the completed (empty buffer and stack of size 1) parses from unfinished_parses\n",
    "end while\n",
    "\n",
    "Return:The dependencies for each (now completed) parse in partial_parses.\n",
    "```\n",
    "---\n",
    "\n",
    "\n",
    "Implement this algorithm in the minibatch parse function in `parser_transitions.py`. You\n",
    "can run basic (non-exhaustive) tests by running python `parser_transitions.py partd`.\n",
    "\n",
    "**Note**: You will need minibatch parse to be correctly implemented to evaluate the model you will\n",
    "build in part (e). However, you do not need it to train the model, so you should be able to complete\n",
    "most of part (e) even if minibatch parse is not implemented yet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def minibatch_parse(sentences, model, batch_size):\n",
    "    \"\"\"Parses a list of sentences in minibatches using a model.\n",
    "\n",
    "    @param sentences (list of list of str): A list of sentences to be parsed\n",
    "                                            (each sentence is a list of words and each word is of type string)\n",
    "    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function\n",
    "                                model.predict(partial_parses) that takes in a list of PartialParses as input and\n",
    "                                returns a list of transitions predicted for each parse. That is, after calling\n",
    "                                    transitions = model.predict(partial_parses)\n",
    "                                transitions[i] will be the next transition to apply to partial_parses[i].\n",
    "    @param batch_size (int): The number of PartialParses to include in each minibatch\n",
    "\n",
    "\n",
    "    @return dependencies (list of dependency lists): A list where each element is the dependencies\n",
    "                                                    list for a parsed sentence. Ordering should be the\n",
    "                                                    same as in sentences (i.e., dependencies[i] should\n",
    "                                                    contain the parse for sentences[i]).\n",
    "    \"\"\"\n",
    "    dependencies = []\n",
    "\n",
    "    ### YOUR CODE HERE (~8-10 Lines)\n",
    "    ### TODO:\n",
    "    ###     Implement the minibatch parse algorithm as described in the pdf handout\n",
    "    ###\n",
    "    ###     Note: A shallow copy (as denoted in the PDF) can be made with the \"=\" sign in python, e.g.\n",
    "    ###                 unfinished_parses = partial_parses[:].\n",
    "    ###             Here `unfinished_parses` is a shallow copy of `partial_parses`.\n",
    "    ###             In Python, a shallow copied list like `unfinished_parses` does not contain new instances\n",
    "    ###             of the object stored in `partial_parses`. Rather both lists refer to the same objects.\n",
    "    ###             In our case, `partial_parses` contains a list of partial parses. `unfinished_parses`\n",
    "    ###             contains references to the same objects. Thus, you should NOT use the `del` operator\n",
    "    ###             to remove objects from the `unfinished_parses` list. This will free the underlying memory that\n",
    "    ###             is being accessed by `partial_parses` and may cause your code to crash.\n",
    "\n",
    "\n",
    "    # init parser\n",
    "    partial_parses = [PartialParse(s) for s in sentences]\n",
    "\n",
    "    # make a shallow copy of partial_parses to track unfinished parses\n",
    "    unfinished_parses = partial_parses[:]\n",
    "    dependencies = [[] for _ in partial_parses]\n",
    "\n",
    "    while unfinished_parses:\n",
    "        for start  in range(0, len(unfinished_parses), batch_size):\n",
    "            end = min(start + batch_size, len(unfinished_parses))\n",
    "            minibatch = unfinished_parses[start:end]\n",
    "            transitions = model.predict(minibatch)\n",
    "            logger.debug(transitions)\n",
    "            for e in range(len(transitions)):\n",
    "                minibatch[e].parse([transitions[e]])\n",
    "\n",
    "        # Remove finished parses from unfinished_parses\n",
    "        remove = []\n",
    "        for p in unfinished_parses:\n",
    "            # if there are not items left in buffer and stack is  just 'ROOT', parse finished\n",
    "            if (not p.buffer) and len(p.stack) == 1:\n",
    "                dependencies[partial_parses.index(p)] = p.dependencies.copy()\n",
    "                remove.append(p)\n",
    "        for p in remove:\n",
    "            unfinished_parses.remove(p)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "We are now going to train a neural network to predict, given the state of the stack, buffer, and\n",
    "dependencies, which transition should be applied next. First, the model extracts a feature vector\n",
    "representing the current state. We will be using the feature set presented in the original neural\n",
    "dependency parsing paper:A Fast and Accurate Dependency Parser using Neural Networks.^4 The\n",
    "function extracting these features has been implemented for you inutils/parserutils.py.\n",
    "This feature vector consists of a list of tokens (e.g., the last word in the stack, first word in the buffer,\n",
    "dependent of the second-to-last word in the stack if there is one, etc.). They can be represented\n",
    "as a list of integers [w 1 ,w 2 ,...,wm] wheremis the number of features and each 0≤wi<|V|is\n",
    "the index of a token in the vocabulary (|V|is the vocabulary size). First our network looks up an\n",
    "embedding for each word and concatenates them into a single input vector:\n",
    "\n",
    "\n",
    "$ x= [Ew 1 ,...,Ewm] \\in R dm$\n",
    "\n",
    "\n",
    "where $E∈R|V|×d $ is an embedding matrix with each rowEwas the vector for a particular word w.\n",
    "\n",
    "(^4) Chen and Manning, 2014,http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf\n",
    "\n",
    "\n",
    "![dependency diagram](./img/model-arch.png)\n",
    "\n",
    "We then compute our prediction as:\n",
    "\n",
    ">$\n",
    "h= ReLU(xW+b 1 ) \\\\\n",
    "l=hU+b 2 \\\\\n",
    "\\hat{y}= softmax(l)\n",
    "$\n",
    "\n",
    "\n",
    "wherehis referred to as the hidden layer,lis referred to as the logits,ˆy is referred to as the\n",
    "predictions, and ReLU(z) = max(z,0)). We will train the model to minimize cross-entropy loss:\n",
    "\n",
    "> $J(θ) =CE(y,yˆ) =− \\sum_i y_i \\cdot \\log(\\hat y_i) $\n",
    "\n",
    "To compute the loss for the training set, we average thisJ(θ) across all training examples.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(e) (10 points) In `parsermodel.py` you will find skeleton code to implement this simple neural net-\n",
    "work using PyTorch. Complete the init ,embedding lookup and forward functions to\n",
    "implement the model. Then complete the train for epoch and train functions within the\n",
    "`run.py` file.\n",
    "\n",
    "Finally execute python run.py to train your model and compute predictions on test data from\n",
    "Penn Treebank (annotated with Universal Dependencies). Make sure to turn off debug setting by\n",
    "settingdebug=False in themainfunction ofrun.py.\n",
    "\n",
    "Hints:\n",
    "\n",
    "- When debugging, setdebug=Truein themainfunction ofrun.py. This will cause the code\n",
    "    to run over a small subset of the data, so that training the model won’t take as long. Make\n",
    "    sure to setdebug=Falseto run the full model once you are done debugging.\n",
    "- When running withdebug=True, you should be able to get a loss smaller than 0.2 and a UAS\n",
    "    larger than 65 on the dev set (although in rare cases your results may be lower, there is some\n",
    "    randomness when training).\n",
    "- It should take about1 hourto train the model on the entire the training dataset, i.e., when\n",
    "    debug=False.\n",
    "- When running withdebug=False, you should be able to get a loss smaller than 0.08 on the\n",
    "    train set and an Unlabeled Attachment Score larger than 87 on the dev set. For comparison,\n",
    "    the model in the original neural dependency parsing paper gets 92.5 UAS. If you want, you\n",
    "    can tweak the hyperparameters for your model (hidden layer size, hyperparameters for Adam,\n",
    "    number of epochs, etc.) to improve the performance (but you are not required to do so).\n",
    "\n",
    "Deliverables:\n",
    "- Working implementation of the neural dependency parser inparsermodel.py. (We’ll look\n",
    "at and run this code for grading).\n",
    "- Report the best UAS your model achieves on the dev set and the UAS it achieves on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class ParserModel(nn.Module):\n",
    "    \"\"\" Feedforward neural network with an embedding layer and single hidden layer.\n",
    "    The ParserModel will predict which transition should be applied to a\n",
    "    given partial parse configuration.\n",
    "\n",
    "    PyTorch Notes:\n",
    "        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks\n",
    "            are a subclass of this \"nn.Module\".\n",
    "        - The \"__init__\" method is where you define all the layers and their respective parameters\n",
    "            (embedding layers, linear layers, dropout layers, etc.).\n",
    "        - \"__init__\" gets automatically called when you create a new instance of your class, e.g.\n",
    "            when you write \"m = ParserModel()\".\n",
    "        - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus,\n",
    "            you should add the \"self.\" prefix layers, values, etc. that you want to utilize\n",
    "            in other ParserModel methods.\n",
    "        - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html.\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings, n_features=36,\n",
    "        hidden_size=200, n_classes=3, dropout_prob=0.5):\n",
    "        \"\"\" Initialize the parser model.\n",
    "\n",
    "        @param embeddings (Tensor): word embeddings (num_words, embedding_size)\n",
    "        @param n_features (int): number of input features\n",
    "        @param hidden_size (int): number of hidden units\n",
    "        @param n_classes (int): number of output classes\n",
    "        @param dropout_prob (float): dropout probability\n",
    "        \"\"\"\n",
    "        super(ParserModel, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.embed_size = embeddings.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.pretrained_embeddings = nn.Embedding(embeddings.shape[0], self.embed_size)\n",
    "        self.pretrained_embeddings.weight = nn.Parameter(torch.tensor(embeddings))\n",
    "\n",
    "        ### YOUR CODE HERE (~5 Lines)\n",
    "        ### TODO:\n",
    "        ###     1) Construct `self.embed_to_hidden` linear layer, initializing the weight matrix\n",
    "        ###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)\n",
    "        ###     2) Construct `self.dropout` layer.\n",
    "        ###     3) Construct `self.hidden_to_logits` linear layer, initializing the weight matrix\n",
    "        ###         with the `nn.init.xavier_uniform_` function with `gain = 1` (default)\n",
    "        ###\n",
    "        ### Note: Here, we use Xavier Uniform Initialization for our Weight initialization.\n",
    "        ###         It has been shown empirically, that this provides better initial weights\n",
    "        ###         for training networks than random uniform initialization.\n",
    "        ###         For more details checkout this great blogpost:\n",
    "        ###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
    "        ### Hints:\n",
    "        ###     - After you create a linear layer you can access the weight\n",
    "        ###       matrix via:\n",
    "        ###         linear_layer.weight\n",
    "        ###\n",
    "        ### Please see the following docs for support:\n",
    "        ###     Linear Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
    "        ###     Xavier Init: https://pytorch.org/docs/stable/nn.html#torch.nn.init.xavier_uniform_\n",
    "        ###     Dropout: https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
    "\n",
    "        self.embed_to_hidden = nn.Linear(self.embed_size * self.n_features, self.hidden_size)\n",
    "        nn.init.xavier_uniform_(self.embed_to_hidden.weight,gain = 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        self.hidden_to_logits = nn.Linear(self.hidden_size, self.n_classes)\n",
    "        nn.init.xavier_uniform_(self.hidden_to_logits.weight, gain = 1)\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def embedding_lookup(self, t):\n",
    "        \"\"\" Utilize `self.pretrained_embeddings` to map input `t` from input tokens (integers)\n",
    "            to embedding vectors.\n",
    "\n",
    "            PyTorch Notes:\n",
    "                - `self.pretrained_embeddings` is a torch.nn.Embedding object that we defined in __init__\n",
    "                - Here `t` is a tensor where each row represents a list of features. Each feature is represented by an integer (input token).\n",
    "                - In PyTorch the Embedding object, e.g. `self.pretrained_embeddings`, allows you to\n",
    "                    go from an index to embedding. Please see the documentation (https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding)\n",
    "                    to learn how to use `self.pretrained_embeddings` to extract the embeddings for your tensor `t`.\n",
    "\n",
    "            @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "\n",
    "            @return x (Tensor): tensor of embeddings for words represented in t\n",
    "                                (batch_size, n_features * embed_size)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE (~1-3 Lines)\n",
    "        ### TODO:\n",
    "        ###     1) Use `self.pretrained_embeddings` to lookup the embeddings for the input tokens in `t`.\n",
    "        ###     2) After you apply the embedding lookup, you will have a tensor shape (batch_size, n_features, embedding_size).\n",
    "        ###         Use the tensor `view` method to reshape the embeddings tensor to (batch_size, n_features * embedding_size)\n",
    "        ###\n",
    "        ### Note: In order to get batch_size, you may need use the tensor .size() function:\n",
    "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.size\n",
    "        ###\n",
    "        ###  Please see the following docs for support:\n",
    "        ###     Embedding Layer: https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
    "        ###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
    "        x = self.pretrained_embeddings(t)\n",
    "        # logger.debug(f'size before reshape {x.size()}')\n",
    "        # print(f'size before reshape {x.size()}')\n",
    "\n",
    "        x = x.view(x.size()[0],-1)\n",
    "        # logger.debug(f'size after reshape {x.size()}')\n",
    "        # print(f'size after reshape {x.size()}')\n",
    "        ### END YOUR CODE\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\" Run the model forward.\n",
    "\n",
    "            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n",
    "\n",
    "            PyTorch Notes:\n",
    "                - Every nn.Module object (PyTorch model) has a `forward` function.\n",
    "                - When you apply your nn.Module to an input tensor `t` this function is applied to the tensor.\n",
    "                    For example, if you created an instance of your ParserModel and applied it to some `t` as follows,\n",
    "                    the `forward` function would called on `t` and the result would be stored in the `output` variable:\n",
    "                        model = ParserModel()\n",
    "                        output = model(t) # this calls the forward function\n",
    "                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n",
    "\n",
    "        @param t (Tensor): input tensor of tokens (batch_size, n_features)\n",
    "\n",
    "        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n",
    "                                 without applying softmax (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ###  YOUR CODE HERE (~3-5 lines)\n",
    "        ### TODO:\n",
    "        ###     1) Apply `self.embedding_lookup` to `t` to get the embeddings\n",
    "        ###     2) Apply `embed_to_hidden` linear layer to the embeddings\n",
    "        ###     3) Apply relu non-linearity to the output of step 2 to get the hidden units.\n",
    "        ###     4) Apply dropout layer to the output of step 3.\n",
    "        ###     5) Apply `hidden_to_logits` layer to the output of step 4 to get the logits.\n",
    "        ###\n",
    "        ### Note: We do not apply the softmax to the logits here, because\n",
    "        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n",
    "        ###\n",
    "        ### Please see the following docs for support:\n",
    "        ###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu\n",
    "        # print(f'size forward reshape {t.size()}')\n",
    "\n",
    "        embed = self.embedding_lookup(t)\n",
    "        hidden = self.embed_to_hidden(embed)\n",
    "        x = self.hidden_to_logits(hidden)\n",
    "        x = self.dropout(x)\n",
    "        logits = F.relu(x)\n",
    "\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "took 3.57 seconds\n",
      "Building parser...\n",
      "took 2.35 seconds\n",
      "Loading pretrained embeddings...\n",
      "took 4.97 seconds\n",
      "Vectorizing data...\n",
      "took 3.46 seconds\n",
      "Preprocessing training data...\n",
      "took 99.77 seconds\n"
     ]
    }
   ],
   "source": [
    "from utils import parser_utils\n",
    "\n",
    "# \n",
    "parser, embeddings, train_data, dev_data, test_data = parser_utils.load_and_preprocess_data(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import parser_model\n",
    "model = parser_model.ParserModel(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParserModel(\n",
       "  (pretrained_embeddings): Embedding(39638, 50)\n",
       "  (embed_to_hidden): Linear(in_features=1800, out_features=200, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (hidden_to_logits): Linear(in_features=200, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring the best model weights found on the dev set\n",
      "Final evaluation on test set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ParserModel(\n",
       "  (pretrained_embeddings): Embedding(39638, 50)\n",
       "  (embed_to_hidden): Linear(in_features=1800, out_features=200, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       "  (hidden_to_logits): Linear(in_features=200, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load trained model for playing\n",
    "\n",
    "model_file = 'path/to/model/model.weights'\n",
    "\n",
    "# assign model to parser\n",
    "parser.model = model\n",
    "    \n",
    "print(\"Restoring the best model weights found on the dev set\")\n",
    "parser.model.load_state_dict(torch.load(model_file))\n",
    "print(\"Final evaluation on test set\",)\n",
    "parser.model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2919736it [00:00, 43968578.48it/s]      \n"
     ]
    }
   ],
   "source": [
    "UAS, dependencies = parser.parse(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze parsed dependencies\n",
    "\n",
    "Let's Take a look at 1st sentence in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1), (7, 6), (7, 5), (7, 4), (7, 3), (7, 2), (7, 8), (0, 7)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dependency prediction on first sentence\n",
    "test_dep = dependencies[0]\n",
    "test_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'head': [-1, 7, 7, 7, 7, 7, 7, 0, 7],\n",
       " 'label': [-1, 1, 27, 23, 25, 8, 15, 0, 27],\n",
       " 'pos': [87, 43, 46, 56, 50, 49, 44, 42, 47],\n",
       " 'word': [39637, 200, 88, 103, 114, 122, 833, 473, 90]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first sentence\n",
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<ROOT>', 'no', ',', 'it', 'was', \"n't\", 'black', 'monday', '.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id2tok to get human readable\n",
    "test_sent = [parser.id2tok[w] for w in test_data[0]['word']]\n",
    "test_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",  -->  no\n",
      "monday  -->  black\n",
      "monday  -->  n't\n",
      "monday  -->  was\n",
      "monday  -->  it\n",
      "monday  -->  ,\n",
      "monday  -->  .\n",
      "<ROOT>  -->  monday\n"
     ]
    }
   ],
   "source": [
    "for h,w in test_dep:\n",
    "    print(test_sent[h], ' --> ', test_sent[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly selected sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: ['<ROOT>', '2', '.', 'that', 'mr.', 'lantos', 'supported', 'the', 'rights', 'of', 'the', 'witnesses', 'to', 'take', 'the', 'fifth', 'amendment', '.']\n",
      "--------------------\n",
      "DEPENDENCIES\n",
      "--------------------\n",
      "\t2  -->  .\n",
      "\tlantos  -->  mr.\n",
      "\tsupported  -->  lantos\n",
      "\tsupported  -->  that\n",
      "\tsupported  -->  2\n",
      "\trights  -->  the\n",
      "\twitnesses  -->  the\n",
      "\twitnesses  -->  of\n",
      "\trights  -->  witnesses\n",
      "\ttake  -->  to\n",
      "\tamendment  -->  fifth\n",
      "\tamendment  -->  the\n",
      "\ttake  -->  amendment\n",
      "\trights  -->  take\n",
      "\tsupported  -->  rights\n",
      "\tsupported  -->  .\n",
      "\t<ROOT>  -->  supported\n"
     ]
    }
   ],
   "source": [
    "def analyze_dep(sentence, dependencies):\n",
    "    print(f'sentence: {sentence}')\n",
    "    print('-'*20)\n",
    "    print('DEPENDENCIES')\n",
    "    print('-'*20)\n",
    "    for h,w in dependencies:\n",
    "        print(f'\\t{sentence[h]}  -->  {sentence[w]}')\n",
    "        \n",
    "\n",
    "def id2words(word_ids, id2tok):\n",
    "    return [id2tok[w] for w in word_ids]\n",
    "        \n",
    "def analyze_random_sent(test_data, parser, dependencies):\n",
    "    rndix = np.random.randint(0, len(test_data))\n",
    "    word_ids = test_data[rndix]['word']\n",
    "    sent = id2words(word_ids, parser.id2tok)\n",
    "    sent_dep = dependencies[rndix]\n",
    "    analyze_dep(sent, sent_dep)\n",
    "    \n",
    "analyze_random_sent(test_data, parser, dependencies) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What does the data look like? How is the data parsed?\n",
    "\n",
    "\n",
    "A sample from `data/dev.conll`\n",
    "\n",
    "```\n",
    "    1\tInfluential\t_\tADJ\tJJ\t_\t2\tamod\t_\t_\n",
    "    2\tmembers\t_\tNOUN\tNNS\t_\t10\tnsubj\t_\t_\n",
    "    3\tof\t_\tADP\tIN\t_\t6\tcase\t_\t_\n",
    "    4\tthe\t_\tDET\tDT\t_\t6\tdet\t_\t_\n",
    "    ...\n",
    "    33\tsale\t_\tNOUN\tNN\t_\t28\tnmod\t_\t_\n",
    "    34\tof\t_\tADP\tIN\t_\t36\tcase\t_\t_\n",
    "    35\tsick\t_\tADJ\tJJ\t_\t36\tamod\t_\t_\n",
    "    36\tthrifts\t_\tNOUN\tNNS\t_\t33\tnmod\t_\t_\n",
    "    37\t.\t_\tPUNCT\t.\t_\t10\tpunct\t_\t_\n",
    "\n",
    "    1\tThe\t_\tDET\tDT\t_\t2\tdet\t_\t_\n",
    "```\n",
    "\n",
    "## Parsing\n",
    "\n",
    "```python\n",
    "\n",
    "    >>>line.strip().split('\\t')\n",
    "    ...\n",
    "    >>>examples.append({'word': word, 'pos': pos, 'head': head, 'label': label})\n",
    "    \n",
    "    >>>examples\n",
    "    [{'word': ['influential', 'members', 'of', 'the',...],\n",
    "       'pos': ['JJ', 'NNS', 'IN', 'DT',...],\n",
    "        'head': [2, 10, 6, 6,...],\n",
    "        'label': ['amod', 'nsubj', 'case', 'det',...]\n",
    "     },\n",
    "     {\n",
    "         ...\n",
    "     }\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_conll in module utils.parser_utils:\n",
      "\n",
      "read_conll(in_file, lowercase=False, max_example=None)\n",
      "    Parse data.\n",
      "    \n",
      "    Parse raw data into python objects for further preprocessing.\n",
      "    \n",
      "    Args:\n",
      "        in_file (str) : path to data\n",
      "        lowercase ( bool)\n",
      "        max_example ( int)\n",
      "    \n",
      "    Returns:\n",
      "        (dict) : {'word': <word>, 'pos': <pos>, 'head': <head>, 'label': <label>}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import utils.parser_utils\n",
    "\n",
    "help(utils.parser_utils.read_conll)\n",
    "\n",
    "# NOTE: use SHIFT-TAB to see docstrings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    class Parser():\n",
    "        ...\n",
    "```\n",
    "\n",
    "    {'root_label': 'root', 'L_NULL': 38, 'unlabeled': True, 'with_punct': True,\n",
    "     'use_pos': True, 'use_dep': False, 'language': 'english', 'n_deprel': 1, \n",
    "     'n_trans': 3, \n",
    "     'tran2id': {'L': 0, 'R': 1, 'S': 2}, \n",
    "     'id2tran': {0: 'L', 1: 'R', 2: 'S'}, \n",
    "     'P_UNK': 82, 'P_NULL': 83, 'P_ROOT': 84, 'UNK': 5154, 'NULL': 5155, 'ROOT': 5156, \n",
    "     'tok2id': {'<l>:root': 0, '<l>:det:predet': 1, '<l>:cc:preconj': 2, '<l>:dobj': 3, '<l>:nmod': 4, ..}\n",
    "     'id2tok': {...}\n",
    "    'n_features': 36, \n",
    "    'n_tokens': 5157}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps \n",
    "\n",
    "\n",
    "- Load data\n",
    "\n",
    "```python\n",
    ">>>train_set = read_conll(...)\n",
    "{'word': \n",
    "     ['in', 'an', 'oct.', '19', 'review', 'of', '``', 'the', 'misanthrope', \"''\", 'at', 'chicago', \"'s\", 'goodman', 'theatre', '-lrb-', '``', 'revitalized', 'classics', 'take', 'the', 'stage', 'in', 'windy', 'city', ',', \"''\", 'leisure', '&', 'arts', '-rrb-', ',', 'the', 'role', 'of', 'celimene', ',', 'played', 'by', 'kim', 'cattrall', ',', 'was', 'mistakenly', 'attributed', 'to', 'christina', 'haag', '.'], \n",
    "\n",
    "'pos': \n",
    "     ['IN', 'DT', 'NNP', 'CD', 'NN', 'IN', '``', 'DT', 'NN', \"''\", 'IN', 'NNP', 'POS', 'NNP', 'NNP', '-LRB-', '``', 'VBN', 'NNS', 'VB', 'DT', 'NN', 'IN', 'NNP', 'NNP', ',', \"''\", 'NN', 'CC', 'NNS', '-RRB-', ',', 'DT', 'NN', 'IN', 'NNP', ',', 'VBN', 'IN', 'NNP', 'NNP', ',', 'VBD', 'RB', 'VBN', 'TO', 'NNP', 'NNP', '.'], \n",
    "\n",
    "'head': \n",
    "     [5, 5, 5, 5, 45, 9, 9, 9, 5, 9, 15, 15, 12, 15, 9, 20, 20, 19, 20, 5, 22, 20, 25, 25, 20, 20, 20, 20, 28, 28, 20, 45, 34, 45, 36, 34, 34, 34, 41, 41, 38, 34, 45, 45, 0, 48, 48, 45, 45], \n",
    "\n",
    "'label': \n",
    "     ['case', 'det', 'compound', 'nummod', 'nmod', 'case', 'punct', 'det', 'nmod', 'punct', 'case', 'nmod:poss', 'case', 'compound', 'nmod', 'punct', 'punct', 'amod', 'nsubj', 'dep', 'det', 'dobj', 'case', 'compound', 'nmod', 'punct', 'punct', 'dep', 'cc', 'conj', 'punct', 'punct', 'det', 'nsubjpass', 'case', 'nmod', 'punct', 'acl', 'case', 'compound', 'nmod', 'punct', 'auxpass', 'advmod', 'root', 'case', 'compound', 'nmod', 'punct']}\n",
    "```\n",
    "\n",
    "- build parser\n",
    "\n",
    "```python\n",
    ">>>parser = Parser(train_set)\n",
    "```\n",
    "    \n",
    "- load word vectors (pretrained embeddings) from `data/en-cw.txt`\n",
    "    initialize \n",
    "    \n",
    "```python \n",
    ">>>embeddings_matrix = np.asarray(np.random.normal(0, 0.9, (parser.n_tokens, 50)), dtype='float32')\n",
    "\n",
    "```\n",
    "\n",
    ">    {'!': [-1.03682, 1.77856, -0.693547, 1.5948, 1.5799, 0.859243, 1.15221, -0.976317, 0.745304, -0.494589, 0.308086, 0.25239, -0.1976, 1.26203, 0.813864, -0.940734, -0.215163, 0.11645, 0.525697, 1.95766, 0.394232, 1.27717, 0.710788, -0.389351, 0.161775, -0.106038, 1.14148, 0.607948, 0.189781, -1.06022, 0.280702, 0.0251156, -0.198067, 2.33027, 0.408584, 0.350751, -0.351293, 1.77318, -0.723457, -0.13806, -1.47247, 0.541779, -2.57005, -0.227714, -0.817816, -0.552209, 0.360149, -0.10278, -0.36428, -0.64853], \n",
    "    ...}\n",
    "    \n",
    "- Vectorize data\n",
    "\n",
    "item | value | vectorized\n",
    "---    | --- | ----\n",
    "'word'| ['in', 'an', 'oct.', '19',... , 'haag', '.'] |[5156, 91, 113, 806, 562,..., 1364, 87]\n",
    "'pos'| ['IN', 'DT', 'NNP', 'CD',..., 'NNP', '.'] | [84, 40, 41, 42, 49,..., 42, 46]\n",
    "'head'| [5, 5, 5, 5,..., 45, 45] |[-1, 5, 5, 5, 5, ..., 45, 45]\n",
    "'label'| ['case', 'det', 'compound', 'nummod', ..., 'nmod', 'punct'] |[-1, 9, 37, 7, 5, ..., 4, 34]\n",
    "\n",
    "   **NOTE** : prepend `self.ROOT` \n",
    "\n",
    "```python\n",
    ">>>train_set = parser.vectorize(train_set)\n",
    "```\n",
    "\n",
    "```python\n",
    "word = [self.ROOT] + [self.tok2id[w] if w in self.tok2id\n",
    "                      else self.UNK for w in ex['word']]\n",
    "pos = [self.P_ROOT] + [self.tok2id[P_PREFIX + w] if P_PREFIX + w in self.tok2id\n",
    "                       else self.P_UNK for w in ex['pos']]\n",
    "head = [-1] + ex['head']\n",
    "label = [-1] + [self.tok2id[L_PREFIX + w] if L_PREFIX + w in self.tok2id\n",
    "                else -1 for w in ex['label']]\n",
    "```\n",
    "\n",
    "- Preprocess data\n",
    "\n",
    "```python\n",
    ">>>parser.create_instances(...)\n",
    "```\n",
    "\n",
    "\n",
    "- Create model\n",
    "```python\n",
    ">>>model = ParserModel(embeddings)\n",
    ">>>model\n",
    "ParserModel(\n",
    "  (pretrained_embeddings): Embedding(5157, 50)\n",
    "  (embed_to_hidden): Linear(in_features=1800, out_features=200, bias=True)\n",
    "  (dropout): Dropout(p=0.5)\n",
    "  (hidden_to_logits): Linear(in_features=200, out_features=3, bias=True)\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "\n",
    "(f) (12 points) We’d like to look at example dependency parses and understand where parsers like ours\n",
    "might be wrong. For example, in this sentence:\n",
    "\n",
    "\n",
    "- Prepositional Phrase Attachment Error: In the example above, the phraseinto Afghanistan\n",
    "    is a prepositional phrase. A Prepositional Phrase Attachment Error is when a prepositional\n",
    "    phrase is attached to the wrong head word (in this example,troopsis the wrong head word and\n",
    "    sentis the correct head word). More examples of prepositional phrases includewith a rock,\n",
    "    before midnightandunder the carpet.\n",
    "- Verb Phrase Attachment Error: In the sentenceLeaving the store unattended, I went\n",
    "    outside to watch the parade, the phraseleaving the store unattendedis a verb phrase. A Verb\n",
    "    Phrase Attachment Error is when a verb phrase is attached to the wrong head word (in this\n",
    "    example, the correct head word iswent).\n",
    "- Modifier Attachment Error: In the sentenceI am extremely short, the adverbextremelyis\n",
    "    a modifier of the adjectiveshort. A Modifier Attachment Error is when a modifier is attached\n",
    "    to the wrong head word (in this example, the correct head word isshort).\n",
    "- Coordination Attachment Error: In the sentenceWould you like brown rice or garlic naan?,\n",
    "    the phrasesbrown riceandgarlic naanare both conjuncts and the wordoris the coordinating\n",
    "    conjunction. The second conjunct (heregarlic naan) should be attached to the first conjunct\n",
    "    (herebrown rice). A Coordination Attachment Error is when the second conjunct is attached\n",
    "    to the wrong head word (in this example, the correct head word isrice). Other coordinating\n",
    "    conjunctions includeand,butandso.\n",
    "In this question are four sentences with dependency parses obtained from a parser. Each sentence\n",
    "has one error, and there is one example of each of the four types above. For each sentence, state\n",
    "the type of error, the incorrect dependency, and the correct dependency. To demonstrate: for the\n",
    "example above, you would write:\n",
    "- Error type: Prepositional Phrase Attachment Error\n",
    "- Incorrect dependency: troops→Afghanistan\n",
    "- Correct dependency: sent→Afghanistan\n",
    "Note: There are lots of details and conventions for dependency annotation. If you want to learn\n",
    "more about them, you can look at the UD website:http://universaldependencies.org.^5\n",
    "However, youdo notneed to know all these details in order to do this question. In each of these\n",
    "cases, we are asking about the attachment of phrases and it should be sufficient to see if they are\n",
    "modifying the correct head. In particular, youdo notneed to look at the labels on the the dependency\n",
    "edges – it suffices to just look at the edges themselves.\n",
    "\n",
    "(^5) But note that in the assignment we are actually using UDv1, see:http://universaldependencies.org/docsv1/\n",
    "\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "You shall submit this assignment on GradeScope as two submissions – one for “Assignment 3 [coding]” and\n",
    "another for ‘Assignment 3 [written]”:\n",
    "\n",
    "1. Run thecollectsubmission.shscript to produce yourassignment3.zipfile.\n",
    "2. Upload yourassignment3.zipfile to GradeScope to “Assignment 3 [coding]”.\n",
    "3. Upload your written solutions to GradeScope to “Assignment 3 [written]”.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (py3ml)",
   "language": "python",
   "name": "py3ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
